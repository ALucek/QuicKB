# Pipeline Stage Control
pipeline:
  from_stage: "CHUNK"    # Options: CHUNK, GENERATE, TRAIN
  to_stage: "TRAIN"

# Path to Knowledgebase Directory
path_to_knowledgebase: "./testing/knowledgebase"

# Base HF Hub credentials
hub_username: "AdamLucek"
hub_token: null  # or rely on HF_TOKEN environment variable

# Chunking config with upload options
chunker_config:
  output_path: "./output/recursive-chunks.json"
  chunker: "RecursiveTokenChunker"
  chunker_arguments:
    chunk_size: 400
    chunk_overlap: 0
    separators: ["\n\n", "\n", ".", "?", "!", " ", ""]
    keep_separator: true
    is_separator_regex: false
    length_type: "character"
  
  upload_config:
    push_to_hub: true
    hub_private: false
    hub_dataset_id: "AdamLucek/quickb"

# Question Generation with upload options
question_generation:
  output_path: "./output/train_data.json"
  litellm_config:
    model: "openai/gpt-4o-mini"
    model_api_base: null
    embedding_model: "text-embedding-3-large"
    embedding_api_base: null
  max_workers: 20
  deduplication_enabled: true
  similarity_threshold: 0.85

  upload_config:
    push_to_hub: true
    hub_private: false
    hub_dataset_id: "AdamLucek/quickb"

# Training config
training:
  model_settings:
    model_id: "nomic-ai/modernbert-embed-base"
    matryoshka_dimensions: [768, 512, 256, 128, 64]
    metric_for_best_model: "eval_dim_128_cosine_ndcg@10"
  
  training_arguments:
    output_path: "./output/modernbert_mtl"
    epochs: 4
    learning_rate: 2.0e-5
    batch_size: 32
    gradient_accumulation_steps: 16
    
  upload_config:
    push_to_hub: true
    hub_private: false
    hub_model_id: "AdamLucek/modernbert-embed-quickb"