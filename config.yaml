# Core Settings
path_to_knowledgebase: "./testing/knowledgebase"
chunker: "RecursiveTokenChunker"
output_path: "./output/knowledgebase-quickb.json"

# Chunking Configuration
chunker_arguments:
  chunk_size: 400
  chunk_overlap: 0
  separators: ["\n\n", "\n", ".", "?", "!", " ", ""]
  keep_separator: true
  is_separator_regex: false
  length_function: "character"

# Question Generation Settings
generate_questions: true
question_output_path: "./output/train_data.json"
deduplication:
  enabled: true
  similarity_threshold: 0.85

# Hugging Face Hub Settings
hub_username: "AdamLucek"  
hub_token: null     # Will use HF_TOKEN environment variable if not specified
hub_private: false

# Training Settings
train_embedding: true
training:
  model_id: "nomic-ai/modernbert-embed-base"
  output_dir: "./output/modernbert_mtl"
  epochs: 4
  learning_rate: 2.0e-5
  matryoshka_dimensions: [768, 512, 256, 128, 64]
  batch_size: 32
  gradient_accumulation_steps: 16
  metric_for_best_model: "eval_dim_128_cosine_ndcg@10"
  push_to_hub: true
  hub_model_id: "AdamLucek/modernbert-embed-quickb"

# ==============================

# # Just chunking
# path_to_knowledgebase: "./testing/knowledgebase"
# chunker: "RecursiveTokenChunker"
# chunker_arguments:
#   chunk_size: 400
#   chunk_overlap: 0
#   separators: ["\n\n", "\n", ".", "?", "!", " ", ""]
# output_path: "./output/kb.json"

# # Disable optional features
# generate_questions: false
# train_embedding: false

# ===============================

# # Chunking + Question Generation
# path_to_knowledgebase: "./testing/knowledgebase"
# chunker: "RecursiveTokenChunker"
# chunker_arguments:
#   chunk_size: 400
# output_path: "./output/kb.json"

# # Enable question generation
# generate_questions: true
# question_output_path: "./output/train.json"
# deduplication:
#   enabled: true
#   similarity_threshold: 0.85

# # Disable training
# train_embedding: false

# ===============================

# # Chunking + Questions + Model Training 
# path_to_knowledgebase: "./testing/knowledgebase"
# chunker: "ClusterSemanticChunker"
# chunker_arguments:
#   embedding_function: "openai"
#   max_chunk_size: 400
#   min_chunk_size: 50
# output_path: "./output/kb.json"

# generate_questions: true
# question_output_path: "./output/train.json"
# deduplication:
#   enabled: true
#   similarity_threshold: 0.85

# train_embedding: true
# training:
#   model_id: "nomic-ai/modernbert-embed-base"
#   output_dir: "./output/model"
#   epochs: 4

# hub_username: "YourUsername"
# hub_private: true