{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abfd742-3602-4d97-894b-28f5f1edfd5a",
   "metadata": {},
   "source": [
    "# QuicKB Integration - ChromaDB\n",
    "\n",
    "This example notebook shows you how to implement your knowledgebase and fine-tuned model with ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dae01d8-72f9-47a8-af45-a57785beec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed:\n",
    "# !pip install chromadb datasets sentence-transformers\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230a0175-978f-4fe0-9aa5-b971f9047a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa45dc1fbf046aa95efef6c123a6109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad23ee7a7c449b8b8d5654432bae8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5027993452f04034b96e47158ae6cb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/30.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf30bcf43fc4ab195e1a98f741e944a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860cd9757831417b8434e6a2972c31cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083b6c9b7d6b435b9e236e4de8cd5ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/596M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1c066ddf0344b493d1ea07e4e54a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1272308a822644639e60b5dd191d000d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c76ce0d6a94821b0972a6a34d79c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f29f0acabad4633a0b8546dfe2f16c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from Hugging Face\n",
    "model_id = \"AdamLucek/modernbert-embed-quickb\"  # Replace with your model ID\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80550160-cca5-4aea-a1c3-6909e1daaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding function\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=model_id,\n",
    "    device=\"cuda\" if model.device.type == \"cuda\" else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24e6709-b8e6-49d4-82d5-019348738503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_quickb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31653903-2adb-4b8f-b8b4-447cabfd71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"quickb_collection\",\n",
    "    embedding_function=ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1f9a1a-d9dd-4981-99bf-f11a52ce01e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5af1a282aeb47b0ba8ecf48471e3ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"AdamLucek/quickb-kb\")  # Replace with your dataset ID\n",
    "chunks = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b585d9-f408-407c-bf24-43b86f9e1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model with `torch.compile` and using a `torch.cpu` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2807 documents\n"
     ]
    }
   ],
   "source": [
    "# Add documents to ChromaDB\n",
    "batch_size = 500\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=batch['text'],\n",
    "        metadatas=[{'source': doc} for doc in batch['source']],\n",
    "        ids=[str(id) for id in batch['id']]\n",
    "    )\n",
    "\n",
    "print(f\"Added {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20e2500-df33-4c9b-bedb-1bd4e71e9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "Distance: 0.3171\n",
      "Source: Al-Hamim_v_Star_2024-12-26.txt\n",
      "Text: self-represented litigants alike have relied on them to draft court filings\n",
      "\n",
      "Result 2\n",
      "Distance: 1.0947\n",
      "Source: Al-Hamim_v_Star_2024-12-26.txt\n",
      "Text: . Some self-represented litigants, including plaintiff, Alim Al-Hamim, have relied on GAI tools to draft court filings, only to discover later to their chagrin that their filings contained hallucinations. Al-Hamim’s opening brief in this appeal contained hallucinations, as well as bona fide legal citations\n",
      "\n",
      "Result 3\n",
      "Distance: 1.1034\n",
      "Source: Al-Hamim_v_Star_2024-12-26.txt\n",
      "Text: .) For these reasons, individuals using the current generation of general-purpose GAI tools to assist with legal research and drafting must be aware of the tools’ propensity to generate outputs 18 containing fictitious legal authorities and must ensure that such fictitious citations do not appear in any court filing\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "results = collection.query(\n",
    "    query_texts=[\"Who has relied on them to draft court filings?\"],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for i, (doc, distance, metadata) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['distances'][0],\n",
    "    results['metadatas'][0]\n",
    ")):\n",
    "    print(f\"\\nResult {i+1}\")\n",
    "    print(f\"Distance: {distance:.4f}\")\n",
    "    print(f\"Source: {metadata['source']}\")\n",
    "    print(f\"Text: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d6648-dd71-4e05-a0b5-bdf7cda1bde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbembed",
   "language": "python",
   "name": "kbembed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
